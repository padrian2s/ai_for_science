<div class="page-content">
  <h2>Section 8: Davis &mdash; The Formalization Revolution</h2>

  <div class="transcript">
    <span class="speaker speaker--guest">Damek Davis:</span>
    <p>I work more in the math-for-AI space in my research, but I have had a strong interest in AI for math for several years. So what I thought I'd try to do with this talk is give you a sense of how I see AI progressing within my own research and share some anecdotes from my own usage over the last couple of years.</p>
    <p>I kind of got interested in this space at the beginning -- when programming languages for formalization like Lean started showing promise. I was trying to figure out whether we could use them in our research. My goal, back in November and December of 2023, was to try to formalize a proof of gradient descent convergence, which is something foundational to my field of optimization.</p>
    <p>I got started, and at the time GPT-4 was the state-of-the-art model. But even formalizing very simple proofs -- where about one line of an English proof takes you fifty lines of Lean code -- was beyond GPT-4's abilities. I spent the rest of December formalizing all but one result that I needed to prove convergence of gradient descent, and then I left the last part as a test. Over the years since November of that year, I would go back and test all the latest models.</p>
    <p>Here's a snapshot from April 2025 where I tested o3, Gemini 2.5 Pro, Claude Code, and others -- nobody was really able to solve this problem.</p>
    <p>I put it aside for a while and came back because there's been a lot of formalization tools developed. So fast-forward to February this year: I gave Aristotle a sketch proof that was generated by GPT-5.2 Pro, and it was able to formalize the result in 15 minutes, in 200 lines of Lean code that are easily readable. It gave me a sense of how rapidly that area has been developing.</p>
    <p>I'll give you a second anecdote. Back in January 2025, a colleague of mine was working on a learning-theory problem. She asked several prominent experts in the field whether they knew a solution -- does it exist somewhere? It looked like something that should exist, but nobody knew.</p>
    <p>So I decided to treat it as an exercise in testing the ability of the LLMs. I read none of the surrounding literature and tried to solve this problem pretty hard for a couple of months just by interacting with o3 and several other LLMs. When I started, o3 couldn't apply definitions correctly -- it was having trouble because the objects we're looking at are very combinatorial in nature and recursively defined, making it difficult for the model to reason about them.</p>
    <p>I worked at this for a while, and by July I was a bit demoralized. My feeling was that the LLMs were destined, for this particular problem, to keep claiming "this is a trivial claim, therefore it's true" -- which was the type of error I kept getting. But I noticed they were pretty good at search: they were able to find some examples and literature that was relevant for me to read, even though they were bad at applying arguments from papers.</p>
    <p>The search aspect turned out to be the key thing, because a week later agent mode was released, and it suggested a list of five papers -- one of which contained the result. It was in the appendix. It was on an unrelated topic. It had many missing keywords that it should have included, and it wasn't really reproducible. But I considered it solved because I knew the solution now.</p>
    <p>I stopped testing the models on this problem, but I decided for this workshop that I would try it again. Seven months later, ChatGPT-5.2 proved the result, cited the literature, and spent 66 minutes thinking about it before it produced anything. It just became much more reliable, and that's sort of my perception in my field of optimization of how things have gone.</p>
    <p>I'll point out though that I did try to use Claude Code -- Claude Opus 4.6 -- today, and Gemini 3 Pro to solve this problem. Claude Code stopped thinking after 30 minutes and crashed twice. Gemini gave me something that was only half correct -- it cited an older 1994 paper that stated but didn't prove the result.</p>
    <p>The last example I wanted to give was about the AlphaEvolve software, which is useful for automated algorithm tuning. I research optimization, and I really like numerical optimization and trying to solve problems quickly with code. Google released this tool called AlphaEvolve -- Terry already mentioned it -- and it was basically a tool for, among other things, optimizing constants in various theorems and proofs.</p>
    <p>I picked an inequality involving the autoconvolution of a function because it looked not so daunting, and I thought: hey, I'm going to claim on Twitter that I'm going to do this live. That was a big mistake because it took over my life for five days. At the end of the five days, I had tried every single thing I could think of. I said: I give up. This problem is hard. There are infinitely many methods I could have tried but didn't because I just don't have the time or motivation.</p>
    <p>It gave me a strong sense that having a lot of compute plus a pretty good prior model that can sample solutions and evolve code is extremely powerful and unpredictable. A lot of the problems we care about -- at least medium-scale problems, as Terry was mentioning -- could probably fall to a lot of compute, let's say a million GPUs, and a pretty good prior model that can search within the space of solutions.</p>
    <p>I think in the next year we're going to see a lot of semi-autonomous results. I'll point to Seb again, who is building the strongest CS theory department in the country at OpenAI. He has a strong team, a strong model, and I suspect in the future we're going to see a lot of semi-autonomous results -- not necessarily autonomous, but within the next year.</p>
    <p>I'll point out that I'm very biased in this because the best performance I've gotten from these models has been in optimization and high-dimensional probability. I think this is a function of maybe the team and the initial training data, so it's not necessarily the case for every field. But that's changing now -- there have been some papers by Axiom Math, an early-stage startup, where they're proving things on Riemann surfaces and some combinatorial identities. These look much more sophisticated than the things we were doing just a year ago.</p>
    <p>Google's doing the same -- some Erdos problems and some things in arithmetic geometry. These are maybe subtasks that you wouldn't normally publish a paper about, but if we're using AI, maybe we are. I would be highly interested to hear from experts on that topic.</p>
    <p>The last thing I'll say is that this is a very good axis for fundraising in the near future for a lot of companies. You can demonstrate capabilities and make this number go up, and we all benefit from it in the sense that we get much more powerful tools. It's very exciting, and I suspect this sort of thing will continue in the near-term future. I don't know about autonomous, but semi-autonomous for sure in the next year.</p>
  </div>

  <div class="commentary">
    <h3>Explained Simply</h3>

    <div class="commentary-section">
      <h4>What Is Formalization, and Why Does It Matter?</h4>
      <p>When mathematicians prove something, they usually write it in a mix of English and mathematical notation that other mathematicians can follow. Formalization means translating that proof into a programming language so precise that a computer can check every single logical step. The language Davis uses is called Lean. If your proof compiles in Lean, it is mathematically correct -- no human judgment needed.</p>
      <p>This matters enormously because mathematical proofs can be long, subtle, and error-prone. Entire published papers have been found to contain mistakes years after publication. A formalized proof is a proof you can trust absolutely. The dream is that AI could take a rough human argument and automatically produce a fully verified formal proof.</p>
    </div>

    <div class="commentary-section">
      <h4>The Dramatic Timeline</h4>
      <p>Davis's personal story is a remarkable illustration of how fast AI is improving. In late 2023, GPT-4 -- the best model available -- could not formalize even simple one-line mathematical arguments in Lean. Davis did most of the work by hand, leaving one stubborn piece as a benchmark. He tested every major new model as it came out: o3, Gemini 2.5 Pro, Claude Code -- none could crack it. Then in February 2026, he gave the problem to Aristotle (a formalization-focused tool) armed with a sketch proof from GPT-5.2 Pro, and it solved the problem in 15 minutes, producing 200 lines of clean, readable Lean code.</p>
      <p>From impossible to solved-in-fifteen-minutes in just over two years. That is the pace of change in this field.</p>
    </div>

    <div class="commentary-section">
      <h4>AI as a Research Librarian</h4>
      <p>Davis's second story reveals a different kind of AI strength. When he spent months trying to get LLMs to solve a learning-theory problem directly, they kept making errors -- confidently declaring that trivial-sounding claims were true when they weren't. But when "agent mode" was released (where the AI can browse, search, and read papers on its own), it found the answer buried in the appendix of an obscure paper on an unrelated topic. The AI was better as a research librarian than as a mathematician -- at least at first. Seven months later, ChatGPT-5.2 could both find and prove the result, spending over an hour reasoning before producing its answer.</p>
    </div>

    <div class="commentary-section">
      <h4>What "Semi-Autonomous" Means for Mathematics</h4>
      <p>Davis is careful with his prediction: he doesn't say AI will do mathematics on its own (autonomous). He says "semi-autonomous" -- meaning AI will increasingly be able to tackle significant mathematical subproblems with minimal human guidance, especially when given enough computing power and a good starting model. The human mathematician still sets the direction, asks the right questions, and validates the results, but the heavy lifting of computation, search, and even proof construction is increasingly handled by AI. He predicts this will become routine within the next year.</p>
    </div>

    <div class="definition-box">
      <strong>Lean (Proof Assistant):</strong> A programming language and software tool designed for writing mathematically rigorous proofs that a computer can verify. Think of it as a spell-checker for logic: if your proof has even a tiny gap or error, Lean will refuse to accept it. Lean has become the most popular proof assistant in the mathematical community, partly due to the ambitious Mathlib project that has formalized a large library of undergraduate and graduate mathematics.
    </div>

    <div class="definition-box">
      <strong>Gradient Descent:</strong> The most fundamental optimization algorithm in machine learning. Imagine you're blindfolded on a hilly landscape and want to find the lowest valley. Gradient descent says: feel which direction slopes downward, take a step that way, and repeat. Nearly every AI model -- from simple linear regression to GPT-5 -- is trained using some variant of gradient descent. Proving that it reliably converges (actually reaches the valley rather than wandering forever) is a foundational result in optimization theory.
    </div>

    <div class="definition-box">
      <strong>AlphaEvolve:</strong> A tool developed by Google that uses large language models to automatically generate and improve code-based solutions to mathematical and algorithmic problems. It works by having the AI propose candidate solutions, evaluating them, and then "evolving" better versions -- similar to natural selection but applied to computer programs. It has been used to discover improved constants in mathematical inequalities and to optimize algorithms, sometimes finding solutions that eluded human researchers.
    </div>

    <div class="highlight-box">
      <strong>Key Insight -- The Dramatic Timeline:</strong> In November 2023, GPT-4 could not formalize even simple one-line proofs in Lean. By April 2025, the best models (o3, Gemini 2.5 Pro, Claude) still could not solve Davis's gradient descent benchmark. Then in February 2026, Aristotle + GPT-5.2 Pro solved it in 15 minutes. Separately, ChatGPT-5.2 independently proved a learning-theory result -- spending 66 minutes in deep reasoning -- that had stumped both human experts and every AI model for over a year. The gap between "completely impossible" and "routine" is closing at a staggering pace.
    </div>
  </div>
</div>
