<div class="page-content">
  <h2>Section 12: Panel 3 -- AI and Higher Education Transformation</h2>

  <div class="transcript">
    <span class="speaker speaker--host">Miguel Garcia (Moderator, UCLA Dean of Physical Sciences):</span>
    <p>Welcome to the final session of our really exciting day. This is the session on higher education transformation -- AI and science in higher education. My name is Miguel Garcia. I'm a professor of chemistry and biochemistry here and also the dean of the division of physical sciences.</p>
    <p>Everything started about June of last year when one day we woke up and 1.2 billion dollars of extramural funding from UCLA disappeared. Our funding was frozen, including the funding for IPAM -- an organization that had just been recommended for funding for another five years. A research enterprise with $1.2 billion in expenditures covers a lot of people -- many in health sciences, physical sciences, engineering -- who rely on that funding for salaries, activities, TAs, postdocs, research scientists.</p>
    <p>This is the context in which we started to talk about this foundation. Chuck's particular interest in AI, the relationship with Terence Tao and Dimitri -- I remember we were having dinner at the Luskin Center and we said, "We need to do something about this." And so the S Foundation came about.</p>
    <p>The foundation intends to create and nurture an ecosystem that brings together leaders in academia, industry, government, foundations, and individual philanthropists. We think there's so much excitement going on in AI right now that this is the right time.</p>
    <p>The role of universities has two components: talent development and deep innovation. These are the two key outcomes. Now, this is called "higher education transformation" -- so what's being transformed? Everything. But today we care about two things: teaching and learning, and research and discovery.</p>
    <p>In teaching and learning, the real concern came from a very sudden disruption. When ChatGPT came out, the concerns were: what is happening to critical thinking and academic integrity? In research, the concern has always been that we cannot compete with industry -- not enough resources, compute, data access, or talent retention when companies are so well-resourced.</p>
    <p>Now let me share some numbers that I think are eye-opening. What is the cost of preparing science and engineering professionals? A college degree in four years is about $320,000, largely paid by families, loans, and financial aid. A PhD costs more, funded by government, foundations, and philanthropy. When you add infrastructure, overhead, research supplies, analytical costs -- a bachelor's degree requires about half a million dollars of total investment, a PhD about $1.1 million, and a postdoc about $1.8 to $2 million. And most of this is taxpayer money.</p>
    <p>Where do all these people go? Apple has 50,000 PhD scientists and engineers. Meta has 40,000. Intel 36,500. Nvidia 20,000. At half a million to $2 million per person, universities steward billions of dollars invested in innovation and human capital by individuals, governments, and corporations. This is a societal investment.</p>
    <p>Is it worth it? The 2024 one-year revenue of four tech companies totals almost $680 billion. The total U.S. investment in STEM is $280 billion. That's an incredible return on investment. If we stop supporting research universities and basic research, the whole cycle collapses.</p>

    <span class="speaker speaker--guest">Bob Jain (University of Pennsylvania):</span>
    <p>Thank you, Miguel. It's been a fascinating day. I'm going to keep my presentation light -- when my AI colleagues invite me to their gatherings, the joke is that I'm the light entertainment because they don't have pictures like astronomers do.</p>
    <p>I'm really excited about teaching. I've taught an intro to AI course for non-STEM students where we teach them how LLMs actually work. I've taught a graduate seminar with a biology colleague and an undergrad machine learning course. I'd love to chat with anybody interested in pedagogy in this age of AI -- all of pedagogy, because I also talk to my humanities colleagues frequently, and they're upset about AI.</p>
    <p>I work with gravitational lensing to study the expanding universe. Our questions are about the origins of galaxies, the nature of dark matter and dark energy. We took images of a hundred million galaxies and studied how the images are distorted as light travels to the telescope. From those distortions, we made the largest map of dark matter made with a galaxy survey. To make the published version, we used diffusion models for the inverse problem from noisy data and got a map four times the resolution. AI accelerates our simulations, lets us generate super-resolution simulations with more complicated physics.</p>
    <p>One of the weird things about AI in cosmology is this: if I use wavelet decomposition, I can capture complex patterns using just a few dozen coefficients. That's powerful -- it means we've understood something. We have a compact representation. But if I use a diffusion model, it would be perfect. The catch? It uses a billion numbers instead of a few dozen. For physicists, that's a terribly non-compact representation. But the magic is that it converges to pretty much the right solution.</p>
    <p>After decades of hyperspecialization, I believe we are on the threshold of a new era of cross-disciplinary science. I'm as excited about how data science and AI are fostering this as I am about how they're accelerating science. From undergraduates to postdocs, we have a way of talking to each other and collaborating that is unprecedented. The most exciting part of my job is talking to people and bringing folks together. My undergraduate student did a project in linguistics, and vice versa.</p>
    <p>The actual path of scientific discovery is still a work in progress. We've heard about types of science but less about how to build experiments, what questions to ask, what to do when a theory doesn't work. There, AI is pretty useful as a cognitive partner -- a different kind of intelligence. But the jury is totally out on whether in a few years we'll have something like AGI that will do this kind of science.</p>

    <span class="speaker speaker--guest">Brenda Rubenstein (Brown University):</span>
    <p>Thank you to the entire S Foundation team and UCLA for hosting this. My expertise is in computational science. I work on machine learning for quantum computing -- and that works both ways. We use quantum computers to train machine learning models so we can understand how chemistry and materials actually function. On the bearish side, error bars on quantum computers are rather large. If we're a little bit clever, they get smaller.</p>
    <p>Our second area is understanding protein and RNA dynamics. If you have a protein that's moving in different ways, how does that cause disease? Proteins involved in cancer -- small mutations change exactly how those proteins move, and that's what actually causes the cancer. We make our own models to predict protein dynamics well into the future. That's an important aspect of science -- we make our own models.</p>
    <p>But what I do for most of my day job is running the Brown Data Science Institute. Many alums come back and say, "Data science is so 2000, why don't you update your name to the AI Center?" We call ourselves the Data Science Institute because we believe data is a larger topic than AI. AI has been built on data, but data is the foundation -- it unites all our disciplines. Even humanists have data.</p>
    <p>My deputy director is actually a historian. What does a historian have to do with AI? Historians are looking at this moment and asking very deep questions about what's going on. It's a historical moment ripe for historians, writers, and many others.</p>
    <p>What do I worry about? The interface between humans and AI -- and I know we've been skirting around this all day. We're coming to a synthesis where humans are interacting with AIs, and we don't know what form that will take. As an educational institution, we have to think carefully. What are our students going to do? What about student welfare? How do we fuse these things together to make both humans and machines better?</p>
    <p>I care about interpretability. It's great if a model can predict things, but if I'm not understanding it, that's a problem. Humans think in one way -- I've been cultured to think as a biologist or physicist. Computers think in a different way. We have to bridge that gap. If I'm going to advance physics, I have to understand what AI is telling me about physics in interpretable ways.</p>
    <p>And the rise of agentic AI is huge for education. We've typically taught undergraduates that they're going to be the expert in the room. But now we're going to have humans with armies of AIs -- or armies of AIs with a few humans. We're going to collaborate with machines in new ways, and humans will have to learn how to lead not just other humans but machines. That's a brand new interface.</p>

    <span class="speaker speaker--guest">Janice Georgeos (USC Dean of Engineering):</span>
    <p>My department chair told me when I first started as a professor that engineering is about three things: energy, materials, and information. AI is actually a combination of all three at an extraordinary level. It's about energy -- we all know how much it consumes. It's about materials -- Nvidia chips, just look at the stock price. And it's about data and information. So AI is almost a manifestation of engineering we've never seen before.</p>
    <p>But there's an additional aspect: trustworthiness. Understanding and interpretability are a very important part of the human element. So to me, AI and engineering now consists of all three things plus trustworthiness.</p>
    <p>I believe what will remain constant in the future is addressing four big problems: sustainable prosperity, health, security, and enriching life. These are the four buckets that will hold true for any discipline. It's very important for our students to have deep technical domain knowledge in these areas. We cannot simply say that all of this will be done by AI.</p>
    <p>If you look at the kinetics of innovation in a very simple way and make the kinetics autocatalytic -- where the rate is proportional to the square of progress rather than just progress itself -- you get a singularity instead of an exponential. Singularity gives power laws, which are associated with phase transitions. I believe we face a phase transition to autonomy.</p>
    <p>I look at AI in terms of three things: AI as a tool, AI as a catalyst, and AI as a technology. A tool is obvious -- you just use it. As a catalyst, it accelerates science and learning dramatically. And as a technology, it represents something profound: traditionally, technology leveraged physical, chemical, or biological phenomena for useful purposes. But AI leverages data for useful purposes. We've removed physics, chemistry, everything -- and all of a sudden, all these techniques become very similar to each other. That's the convergence we're seeing.</p>
    <p>A commitment we're making to our entering undergraduates at USC is that by graduation, they will have a working knowledge of AI -- understanding the mechanics of AI -- whether they're in CS, electrical engineering, chemical engineering, or anything else. It's fundamental to demystify what AI is.</p>

    <span class="speaker speaker--host">Miguel Garcia (Moderator):</span>
    <p>How is AI changing engineering and science education?</p>

    <span class="speaker speaker--guest">Brenda Rubenstein (Brown University):</span>
    <p>I wouldn't limit it to science and engineering education. AI is making us become experts in our field -- and perhaps we should have been experts already. In the past, education in certain STEM fields was about producing people with a very specific skill set but who maybe didn't rise to the level of expert. We produced a lot of CS degrees, a lot of applied math degrees focused on "use this process and do something with it."</p>
    <p>What AI is making us think about is: how do we develop the expertise needed to guide AI, make decisions about AI, and verify AI? We're asking that students know enough to have an image in their heads of how things need to work. That also means being able to communicate what you think needs to work, and being able to lead. AI is forcing us to create experts at a much higher level who can integrate skills much more deeply than before.</p>

    <span class="speaker speaker--guest">Bob Jain (University of Pennsylvania):</span>
    <p>When I talked about abolishing departments, I meant the boundaries between departments. I think it's time to make boundaries much more porous and think about experimental science versus theoretical science versus data analysis, with training that crosses departmental boundaries.</p>
    <p>I think undergraduate students can achieve a tenfold increase in participation in research. Graduate students really need to learn AI methods as soon as they reach grad school. Faculty desperately need to upskill in AI -- otherwise the whole enterprise gets held back by university structure and what our faculty don't yet know. And universities could really learn from tech companies in how to be nimble and change on a timescale of six months to one year rather than decades.</p>

    <span class="speaker speaker--guest">Janice Georgeos (USC Dean of Engineering):</span>
    <p>What I'm trying to understand is: what role will be played by what is conventionally a teaching assistant today? Is the teaching assistant going to be replaced by AI? And how do we judge talent development at the graduate level? Right now you publish two or three papers and graduate. Maybe because of the acceleration of science, you publish those in a year. We may have to change the way we think about what it means to mentor and create researchers of this caliber.</p>

    <span class="speaker speaker--host">Miguel Garcia (Moderator):</span>
    <p>With rapid changes and the need for large resources, how should university research evolve?</p>

    <span class="speaker speaker--guest">Brenda Rubenstein (Brown University):</span>
    <p>The role of a university is as a place for innovation and original ideas -- a test bed. Historically, a cheap test bed. You could think about this idea, that idea, crazy ideas. A lot of those ideas don't have to mesh with reality. Many don't work. But they're ideas, and they teach people. They move students forward. It's important that we remain a test bed so that the best ideas go to industry and other places. And it's important to expose students to how to develop ideas -- that's also a key skill.</p>

    <span class="speaker speaker--guest">Bob Jain (University of Pennsylvania):</span>
    <p>We classify research as 6.1, 6.2, 6.3, 6.7. Traditionally, universities do 6.1, maybe 6.2. Industry goes in the other direction. I think this has been compressed dramatically -- both universities and industry now work in the 6.2, 6.3 area. The 6.1 -- truly basic research -- will probably continue being a property of universities because industry is not necessarily interested in that.</p>
    <p>I have a dream: before Newton, we did not understand nature. Then Newton introduces calculus, F equals ma, and all of a sudden the world makes sense. Is it possible that we're in an era where we haven't discovered the equivalent of Newton today -- something that would allow us to understand what AI does?</p>

    <span class="speaker speaker--host">Miguel Garcia (Moderator):</span>
    <p>What is lacking in or missing from the current AI-for-science discussion?</p>

    <span class="speaker speaker--guest">Bob Jain (University of Pennsylvania):</span>
    <p>I have a colleague, Desmond Patton, who wrote an article on the "joy of AI." Reflecting on collaborative science with AI -- as both a partner to the individual and as a mediator to the group -- is something we have only begun to do. AI immediately takes out a lot of the relative drudgery of writing and coding. So we are free to imagine more. "Joyous" is not a term a lot of physicists love to use, but that mode of science is now open to us along with making more rapid progress.</p>

    <span class="speaker speaker--guest">Brenda Rubenstein (Brown University):</span>
    <p>It's the ensemble nature of things. We can have an ensemble of AIs, people, and others working together in a collaborative form that wasn't there before. For people who don't use a lot of AI, they don't quite see this. But now we're able to bring people onto similar planes and have conversations they weren't able to have before. That's harnessing the full community to solve problems in ways we haven't been able to in the past.</p>

    <span class="speaker speaker--guest">Janice Georgeos (USC Dean of Engineering):</span>
    <p>What is going to become a big, important issue is: what does it mean to publish? What does it mean to use AI for publications? The rules we have right now are primitive. There's going to be a huge revolution in this area that we don't even know the shape of yet.</p>

    <span class="speaker speaker--host">Audience Member:</span>
    <p>How do you take something so all-encompassing and translate it to your faculty, students, and administration -- so it's transformative but still accessible?</p>

    <span class="speaker speaker--guest">Brenda Rubenstein (Brown University):</span>
    <p>We've been through this before with computers. Some people didn't want to use computers, didn't want to deal with Wikipedia, didn't want to deal with Google. Our strategy at Brown is to make sure people understand that AI is here, it is not leaving, it is not going away. You're going to be immersed in it just like everybody uses Google. We put out the tools in many different places -- conversations, forums, projects. In higher ed, we can't knock on doors and say "you have to do this today" because then people don't come to those doors anymore. You put it out there, people absorb it in the ways they're going to absorb it, and they incorporate it into their disciplines. Nobody can tell you how to incorporate it into your discipline.</p>

    <span class="speaker speaker--guest">Janice Georgeos (USC Dean of Engineering):</span>
    <p>I'm not sure it's really very difficult. It's a matter of making a decision that this is what we need to do. Many universities are acutely aware of the need. There is a palpable anxiety about AI -- there's no question about that. But administrators get it. You talk to presidents or provosts, and they understand this is a very important transformative moment. And it changes very fast -- when I was preparing my presentation for the National Academy, I was looking at my slides every day and had to drop some and add new ones because something new in AI came out.</p>

    <span class="speaker speaker--host">Miguel Garcia (Moderator):</span>
    <p>I think that's a really good point to stop. AI is here to stay. Everything presented today tells us this needs to be embraced. Let me invite Dimitri for closing remarks.</p>

    <span class="speaker speaker--host">Dimitri (IPAM Director):</span>
    <p>I just wanted to thank the panelists, Miguel, Chuck, and Terry for a wonderful day. And all of you for coming. There are lots of interesting things happening at IPAM, so please look at our calendar and I hope to see all of you in the future. Thank you so much.</p>
  </div>

  <div class="commentary">
    <h3>Explained Simply</h3>

    <div class="commentary-section">
      <h4>The Stakes for Universities</h4>
      <p>This closing panel addresses something that doesn't get enough attention in AI conversations: what happens to the universities that train the scientists and engineers who build AI in the first place? Dean Miguel Garcia opens with a sobering reality check. In June of the previous year, $1.2 billion in federal research funding at UCLA was suddenly frozen -- a funding crisis that directly threatened the institute hosting this very conference. The numbers he presents are striking: it costs about $1.1 million in total societal investment to produce a single PhD, and about $2 million for a postdoc. The tech companies that hire these graduates generate hundreds of billions in annual revenue. The message is clear -- universities are the foundation of the entire innovation pipeline, and that foundation is under threat.</p>
    </div>

    <div class="definition-box">
      <strong>6.1 / 6.2 / 6.3 Research:</strong> A classification system for types of research. "6.1" is basic research -- driven by curiosity with no immediate practical application. "6.2" is applied research -- directed toward solving specific problems. "6.3" is advanced technology development -- taking proven concepts and turning them into practical systems. Bob Jain's point is that AI has compressed this spectrum: universities and industry now compete in the same middle ground.
    </div>

    <div class="definition-box">
      <strong>Agentic AI:</strong> AI systems that can take independent actions, use tools, make decisions, and pursue goals with minimal human supervision -- as opposed to AI that simply responds to prompts. Brenda Rubenstein flags this as a transformative shift for education: students will need to learn how to lead and manage teams that include AI agents, not just other people.
    </div>

    <div class="definition-box">
      <strong>Phase Transition (in Georgeos's analogy):</strong> A concept from physics where a system suddenly changes from one state to another -- like water turning to ice. Georgeos argues that the pace of AI innovation is not just exponential (doubling regularly) but autocatalytic (feeding on itself and accelerating), which leads not to smooth growth but to a sudden, dramatic shift. He believes we are approaching such a shift toward AI autonomy.
    </div>

    <div class="commentary-section">
      <h4>What Students Actually Need to Learn</h4>
      <p>The panelists converge on a counterintuitive point: in the age of AI, human expertise matters more, not less. Rubenstein argues that AI is exposing a gap in education -- too many graduates had specific technical skills without deep expertise. Now, the ability to verify AI outputs, understand what AI is really telling you, and communicate that understanding to others has become essential. Georgeos goes further, committing that every USC engineering graduate will have a working knowledge of AI regardless of their specific discipline. Bob Jain calls for making departmental boundaries more "porous" and dramatically expanding undergraduate participation in research.</p>
    </div>

    <div class="highlight-box">
      <strong>Key Insight:</strong> The panelists unanimously argue that AI does not reduce the need for human understanding -- it raises the bar. When AI can handle routine analysis, the uniquely human contributions become higher-level: asking the right questions, interpreting results in context, bridging disciplines, and exercising judgment. Universities need to train for these higher-order skills rather than the procedural tasks that AI can now automate.
    </div>

    <div class="commentary-section">
      <h4>The University as Sandbox</h4>
      <p>Rubenstein's vision of the university as a "test bed for ideas" -- including crazy ones that might not work -- captures something important about why basic research matters. Industry optimizes for known problems with clear returns. Universities can afford to explore ideas with no obvious application. This distinction is under pressure as funding shrinks and timescales compress, but the panelists argue it is more important than ever. If universities abandon basic research to chase applied AI projects, the pipeline of truly novel ideas -- the kind that eventually create entire industries -- dries up.</p>
    </div>

    <div class="commentary-section">
      <h4>The Publishing Crisis</h4>
      <p>Georgeos raises a question that may seem niche but has enormous implications: what does "publishing a paper" even mean when AI can generate text, run analyses, and even propose hypotheses? The current rules for AI use in academic publishing are primitive, and the panel suggests a revolution is coming. This connects to Bubeck's earlier point (from the industry panel) that scientists should write fewer but more meaningful papers focused on genuine understanding rather than incremental results.</p>
    </div>

    <div class="quote-box">
      <strong>Notable Quote:</strong> "AI is forcing us to create experts at a much higher level who can integrate skills much more deeply than before. We're asking that students have an image in their heads of how things need to work -- and be able to communicate that." -- Brenda Rubenstein, Brown University
    </div>

    <div class="quote-box">
      <strong>Notable Quote:</strong> "Before Newton, we did not understand nature. Then Newton introduces calculus and all of a sudden the world makes sense. Is it possible that we haven't discovered the equivalent of Newton today -- something that would allow us to understand what AI does?" -- Bob Jain, University of Pennsylvania
    </div>

    <div class="quote-box">
      <strong>Notable Quote:</strong> "Universities steward billions of dollars invested in innovation and human capital. The total 2024 revenue of just four tech companies is almost $680 billion. The total U.S. investment in STEM is $280 billion. That's an incredible return on investment. If we stop supporting research universities, the whole cycle collapses." -- Miguel Garcia, UCLA
    </div>

    <div class="highlight-box">
      <strong>Key Insight:</strong> The conference ends where it began -- with a call for collaboration. But by this point, the urgency is much clearer. Federal research funding is under threat. Universities cannot match industry resources. The talent pipeline that feeds the entire innovation economy depends on publicly funded basic research. The S Foundation's mission -- building bridges between academia, industry, and philanthropy -- is not just aspirational. For institutions like IPAM and UCLA, it may be existential.
    </div>
  </div>
</div>
