<div class="page-content">
  <h2>Section 9: Needell &amp; Bertozzi &mdash; Fair ML, DNA Design &amp; Math for AI</h2>

  <div class="transcript">
    <span class="speaker speaker--guest">Deanna Needell:</span>
    <p>I'm going to keep mine very brief so we can get to the Q&amp;A. I work with a lot of amazing folks -- this is my group in the lower left and a bunch of other programs including REUs that have been run here at UCLA.</p>
    <p>I wanted to give a brief overview of all the areas I work in. I work in optimization, like a few folks have mentioned. I work in particular on methods for large-scale linear systems. I do topic modeling, feature selection, image segmentation -- a lot of imaging and signal processing, prediction, and many more machine learning tasks.</p>
    <p>I also work on the foundational side of ML, which hasn't been talked about as much in the previous two talks -- proving things about the behaviors of ML. Things like benign overfitting, growing, debiasing. A lot of that is actually joint with Guido Montufar, who's also at UCLA and not able to be here today, as well as many of our students.</p>
    <p>I also work in fair machine learning -- tracking bias and error propagation throughout these methods, as well as developing mitigation strategies to make methods fair. Obviously the word "fair" itself is a little bit hard to define, so we work with people in philosophy and other areas to actually make that more of a mathematical definition.</p>
    <p>I also work with several community nonprofits in the medical space. LymeDisease.org is a nonprofit working to study Lyme disease with large data, and the Innocence Center is a nonprofit of attorneys who work to free innocent people from prison. We use LLMs and machine learning to study a lot of these applications as well, and those tend to motivate science -- it's this nice feedback loop that I really enjoy.</p>
    <p>This top picture is showcasing the idea of benign overfitting. We're trying to actually prove and understand when a model will exhibit benign overfitting. This is a visualization of fitting points to a curve. In each of these three examples, clearly the data has been overfit -- the curves in all three examples pass through all the points. But they're overfit in different ways.</p>
    <p>On the left, we consider that benign because if you use that turquoise curve to make a prediction, you're not going to be too far away from the dotted gray line -- that's the truth. Versus this catastrophic overfitting on the right, which is the extreme. I don't even know where these curves go -- they're in the basement underneath the ground somewhere. That would obviously lead to very poor prediction. We've actually developed theory for low-layer neural networks to describe when each of these happens. The idea is that we hope this motivates what happens in the more complex models.</p>
    <p>On the bottom, we've got examples of doing machine learning for feature extraction. We've built this synthetic data -- a population consisting of different groups. Each of the groups experiences very different levels of reconstruction, so this is sort of an unfair model where, for example, a minority group might have terrible prediction whereas a majority group experiences really high prediction. We've mitigated that lack of fairness using a method where all groups experience some levels of accuracy. This is the type of fairness work that we do.</p>
    <p>In terms of working with the nonprofits: on the upper left is a heat map looking at Lyme disease data. This is one of the fastest-growing epidemics in the world and a very poorly understood disease. It kind of mimics long COVID and all these long illnesses that are post-infection. Studying the diagnosis, treatment, and patient response is something we're getting a lot of funding for now. We're using a lot of machine learning to make conclusions about particularly the neurological manifestations of the disease.</p>
    <p>And at the Innocence Center -- this is a fantastic organization that has freed numerous innocent people from prison. They have to look through vast amounts of data: court case data, arrest data, forensic evidence -- all just to make a decision whether to investigate a case or not. It typically takes an attorney about nine months just to make that decision. So you can clearly see there's a nice use of AI to help make that decision and run alongside the human being in analyzing this, and also to understand how the humans themselves are making decisions. I'll wrap up there for sake of time.</p>

    <span class="speaker speaker--guest">Andrea Bertozzi:</span>
    <p>Hi everybody. I'm Andrea Bertozzi. I'm a member of the math department here at UCLA and I have a few slides to share.</p>
    <p>I want to start with an example. I realize we're supposed to talk about math, but I really want to talk about applications. This is a problem I've been working on with the Andrews group in chemistry here at UCLA regarding the selection of DNA aptamers. These are single-stranded DNA molecules, fairly short, and the problem is really a design problem: how do you come up with a seemingly random sequence of nucleotides that, in the form of a small DNA molecule, will fold up in some way and then bind to a target with high affinity and specificity?</p>
    <p>The way this is currently done, using a method called Systematic Evolution of Ligands by Exponential Enrichment -- SELEX for short -- is that it is literally looking for a needle in a haystack. You create a soup of random sequences, like billions of these things, and then over an iterative process of testing them against a target, seeing what sticks, running PCR amplification on that information, and redoing the whole experiment about twenty times, you down-select from about a billion random sequences to maybe 5,000. And that's still way too many to pick one.</p>
    <p>Then you have to come up with a way to identify maybe a couple of candidates to test in the lab. The way this is currently done is by looking at the total count of each sequence at the end of the SELEX process and picking the high-count molecules.</p>
    <p>What we've been doing is using AI -- and in particular more classical machine learning ideas -- to take that data and sort through the secondary structures of the DNA. You can see pictures of different configurations and how they fold up. What you find is, for example, here's a high-count aptamer from the SELEX process, and here's one that shows up just one time, but it's almost identical in structure. So one might ask: is looking only at the high-count molecules the right way to do this, or do we really need to get into the weeds and start understanding more about what comes out of this process?</p>
    <p>Without giving away lots of secret information, I'll just say that yes, there is something to this question -- maybe we can find molecules that we wouldn't have thought to identify by actually using machine learning and AI to understand much better the space of this problem.</p>
    <p>There's a lot of nice mathematics that goes into this. It's not a black-box AI. We're actually using a fair amount of sophisticated mathematics, including ideas from subgraph matching. The similarity structure is designed using something called Motzkin paths, which come from combinatorics. That's a combinatorial object that gives us a one-to-one correspondence between the shape of the DNA and the Motzkin path, and that allows us to use it as a feature vector for machine learning. Then you can use this for computational chemistry.</p>
    <p>The moral of the story is that there's a lot more to using AI than just taking black-box language models and neural networks and plugging in things to see what comes out. You can really get into the weeds and see how to use some very nice ideas from mathematics and computer science to solve some really interesting problems.</p>
    <p>I want to talk about another way we can take ideas from classical mathematics to develop models for machine learning. This is work I've been doing for going on about fifteen years now. The idea is to take classical ideas from surface tension, fluid dynamics, and minimal surface problems -- classical problems in low-dimensional Euclidean space. All of these problems algorithmically have the Laplacian playing a major role. When you want to compute these things numerically, you often use pseudospectral methods where the Laplacian operator diagonalizes because sines and cosines are eigenfunctions of the Laplacian, which allows us to use the Fast Fourier Transform to go back and forth between frequency space and spatial space to very quickly solve minimal surface problems.</p>
    <p>It turns out there's a very beautiful analogy between these things and looking at high-dimensional discrete data organized on a graph. You take your data and put it on a similarity graph where each piece of data is a node, and the edges describe how similar the data points are. Then you can solve something like a graph min-cut problem -- a classical problem in computer science -- but we can look at this as a discrete analog of the minimal surface problem in Euclidean space.</p>
    <p>All these beautiful algorithms from Euclidean space have analogs in the discrete problem. We don't have the FFT, but we can project onto eigensubspaces of the graph Laplacian. Because similarity graphs have so much redundancy, you don't need the full spectrum -- you need things like low-rank approximations or sparse approximations. We often find that we only need a small percentage of the spectral modes to answer questions like the graph min-cut problem.</p>
    <p>These methods are excellent for data reduction in training for semi-supervised and unsupervised learning -- a nice replacement for supervised neural network methods in cases where you just don't have a lot of training data. I'm working with a team at Los Alamos National Lab where there's a lot of data but we just don't have training labels. One example is surface water detection in multispectral images -- information that lives in parts of the world that are hard to get to, like the poles.</p>
    <p>These mathematical ideas let you prove theorems, but they also partner very well with modern neural network methods. One example we've used recently is contrastive learning -- SimCLR -- for dimension reduction of the data.</p>
    <p>The last problem I want to talk about is one showing up in many interesting contexts, including what Damek spoke about -- combinatorial complexity. I'm also starting to work on a DARPA program, the A3ML program on anti-money-laundering, where we're interested in finding patterns of activity -- which is a subgraph matching problem.</p>
    <p>What we discover is that when you have a very large world space of information and some pattern you're trying to find, if you can find one instance, there are probably many instances. Finding just one instance is probably not going to answer the question you need answered. Most people, when they hear "combinatorial complexity," have a knee-jerk reaction to just walk away from that problem. But why? Why not attack the combinatorial complexity head-on?</p>
    <p>It turns out you can make very rigorous definitions of structural similarity and subgraph matching, prove theorems about it, and derive combinatorial formulas for counting all the instances. We have an example with over 10^100 solutions found in real computational time just by taking advantage of structural equivalence.</p>
    <p>Here's a very relevant example from something we worked on with DARPA about five years ago. The left-hand figure is a template for a problem from the biosciences related to the cytokine storm in a COVID-19 infection. We were looking at a very large knowledge graph from lots of papers in the medical literature, and we were asked: can we find any instance of this template in this giant knowledge graph? My group was able to show there were on the order of 10^18 matches. You might think: how can you even begin to get your head around 10^18? Well, you can -- because you can write down a Venn diagram that shows exactly how nodes in the template map to groups.</p>
    <p>All of this can be very nicely paired with large language models and used in the same way for looking at mathematical proofs -- something I hope to be working on in the near future.</p>
    <p>I'm going to finish by saying there's a lot of beautiful mathematical ideas -- constructs that we can actually get our heads around and prove theorems about -- that are very important as pieces of solving these puzzles with AI and language models and all of the things people have talked about today.</p>
  </div>

  <div class="commentary">
    <h3>Explained Simply</h3>

    <div class="commentary-section">
      <h4>Benign Overfitting: When Memorizing Is Okay</h4>
      <p>One of the great mysteries of modern machine learning is that models often "overfit" -- they memorize the training data exactly, passing through every single data point -- and yet they still make good predictions on new data. Classical statistics says this shouldn't work: a model that memorizes noise should perform terribly. Needell's group is working to understand when overfitting is "benign" (harmless) versus "catastrophic" (disastrous).</p>
      <p>Imagine fitting a curve through scattered dots on a graph. Three different curves might all pass through every dot, but one wiggles gently (benign), one wiggles moderately (tempered), and one goes completely haywire between the dots (catastrophic). Needell's team has developed mathematical theory for simple neural networks that predicts which type of overfitting will occur -- providing a foundation for understanding why much larger models like GPT can memorize their training data and still generalize well.</p>
    </div>

    <div class="commentary-section">
      <h4>Fair Machine Learning: When AI Treats People Differently</h4>
      <p>When an ML model is trained on data from a population with different demographic groups, it can end up being very accurate for majority groups and terrible for minority groups -- even if nobody intended that outcome. Needell's work on fair ML tackles this head-on: measuring how errors propagate differently across groups and developing methods to mitigate these disparities. The tricky part is that "fairness" itself doesn't have a single mathematical definition -- what's fair depends on context, values, and trade-offs -- so the team works with philosophers and ethicists to formalize different notions of fairness that can be built into algorithms.</p>
    </div>

    <div class="commentary-section">
      <h4>The DNA Aptamer Problem: A Needle in a Billion Haystacks</h4>
      <p>Bertozzi's DNA aptamer work is a beautiful example of math-powered AI in action. An aptamer is a short strand of DNA that folds into a specific 3D shape and sticks to a target molecule -- like a custom-designed molecular Velcro strip. The problem is that there are billions of possible sequences, and traditional methods (SELEX) narrow them down through brute-force experimental evolution over twenty rounds, ending up with thousands of candidates. Then researchers just pick the ones that appeared most frequently, hoping those are the best.</p>
      <p>Bertozzi's insight is that counting frequency is not enough. A molecule that appears only once might have an almost identical 3D structure to a high-frequency winner. To capture this, her team uses Motzkin paths -- a concept from combinatorics -- to mathematically represent the folding pattern of each DNA strand. This creates a precise "fingerprint" for each molecule's shape that can be used as input to machine learning, letting the AI find structurally similar candidates that brute-force counting would miss entirely.</p>
    </div>

    <div class="commentary-section">
      <h4>Why Math Is Still Essential</h4>
      <p>A consistent theme from both speakers is that the most effective AI for science is not a black box. Bertozzi's work on graph-based methods for semi-supervised learning draws on fifteen years of classical mathematics -- surface tension, the Laplacian, Fourier transforms -- and adapts these ideas to work on high-dimensional data organized as graphs. These methods are especially powerful when you have lots of data but very few labels (someone has to manually label each data point, which is expensive). The mathematical foundations also mean you can prove theorems about when and why these methods work, something that is much harder with pure deep-learning approaches.</p>
      <p>The message from both Needell and Bertozzi is clear: mathematics is not being replaced by AI. Rather, mathematics is the language that makes AI powerful, interpretable, and trustworthy.</p>
    </div>

    <div class="definition-box">
      <strong>Benign Overfitting:</strong> A phenomenon where a machine learning model perfectly memorizes its training data -- fitting every single point exactly -- and yet still makes accurate predictions on new, unseen data. This contradicts the classical statistical wisdom that memorizing data should lead to poor generalization. Understanding when overfitting is benign versus catastrophic is one of the key open questions in modern ML theory.
    </div>

    <div class="definition-box">
      <strong>SELEX (Systematic Evolution of Ligands by Exponential Enrichment):</strong> A laboratory technique used to find DNA or RNA molecules (aptamers) that bind tightly to a specific target. The process starts with a pool of billions of random sequences, repeatedly tests them against the target, amplifies the ones that stick, and repeats -- mimicking evolution by natural selection. After about twenty rounds, the pool is narrowed from billions to thousands of candidates.
    </div>

    <div class="definition-box">
      <strong>Aptamer:</strong> A short, single-stranded DNA or RNA molecule (typically 20-80 nucleotides long) that folds into a specific three-dimensional shape and can bind to a target molecule with high specificity -- similar to how an antibody recognizes a pathogen. Aptamers are sometimes called "chemical antibodies" and have applications in diagnostics, drug delivery, and biosensors.
    </div>

    <div class="definition-box">
      <strong>Motzkin Path:</strong> A concept from combinatorics -- a path on a grid that starts at the origin, takes steps that go up, down, or stay level, and never dips below the starting line. Bertozzi's team uses Motzkin paths to create a one-to-one mathematical representation of how a DNA strand folds: each step in the path corresponds to a structural feature of the molecule (a paired bond going up, an unpaired nucleotide staying level, and so on). This transforms the messy biological problem of comparing molecular shapes into a clean mathematical problem of comparing paths.
    </div>

    <div class="highlight-box">
      <strong>Key Insight -- The Innocence Center:</strong> Needell's work with the Innocence Center -- a nonprofit that frees wrongly convicted people from prison -- puts a human face on AI's potential. Attorneys at the center must wade through enormous volumes of court records, arrest data, and forensic evidence just to decide whether a case is worth investigating. This triage process alone typically takes about nine months per case. By applying LLMs and machine learning to help analyze this data, Needell's team is dramatically accelerating a process where every month of delay is another month an innocent person spends behind bars.
    </div>
  </div>
</div>
